# ðŸ›’ Azure Retail Analytics â€” End-to-End ELT Pipeline
### Medallion Architecture (Bronze â†’ Silver â†’ Gold) using Azure Synapse Analytics

---

## ðŸ“Œ Project Overview

This project demonstrates a complete, production-style **ELT (Extract, Load, Transform) data pipeline** built on **Azure Synapse Analytics**. Raw retail transaction data is ingested from an HTTP source, cleaned and transformed using **PySpark notebooks**, and finally exposed as a queryable **SQL External Table** using Synapse Serverless SQL Pool.

The pipeline follows the **Medallion Architecture** pattern â€” a best practice in modern data engineering â€” ensuring data quality and reliability at every stage before it reaches business users.

---

## ðŸ—ï¸ Architecture

```
[HTTP Source â€” Raw JSON Data]
            â”‚
            â–¼
[Copy Data Pipeline â€” Synapse Integrate Tab]
            â”‚
            â–¼
[ðŸ¥‰ Bronze Layer â€” ADLS Gen2 /bronze folder]
   Raw Parquet files, no transformations
            â”‚
            â–¼
[ðŸ¥ˆ Silver Layer â€” PySpark Notebook]
   Filter: purchases only
   Clean: drop nulls (customer_id, amount)
   Transform: fix timestamps, cast data types
            â”‚
            â–¼
[ðŸ¥‡ Gold Layer â€” PySpark Notebook]
   Aggregate: daily_revenue, total_purchases per day
   Output: clean Parquet ready for reporting
            â”‚
            â–¼
[Serverless SQL Pool â€” External Table]
   SELECT * FROM daily_revenue
```

> ðŸ“ Architecture diagram image coming soon

---

## ðŸ› ï¸ Tech Stack

| Tool | Purpose |
|---|---|
| Azure Synapse Analytics | Core platform â€” pipelines, notebooks, SQL |
| Azure Data Lake Storage Gen2 | Storage for all three layers |
| Apache Spark (PySpark) | Silver and Gold layer transformations |
| Synapse Serverless SQL Pool | SQL presentation layer |
| Parquet File Format | Columnar storage for efficient querying |
| Azure Blob Storage | Source storage account |
| Database Master Key + Scoped Credentials | Secure storage authentication |

---

## ðŸ“‚ Project Structure

```
azure-synapse-retail-analytics/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ architecture-diagram.png
â”‚
â”œâ”€â”€ 01_pipeline/
â”‚   â””â”€â”€ bronze_ingestion_pipeline.json    # Copy Data pipeline export
â”‚
â”œâ”€â”€ 02_notebooks/
â”‚   â”œâ”€â”€ silver_cleaning.ipynb             # PySpark cleaning notebook
â”‚   â””â”€â”€ gold_aggregation.ipynb            # PySpark aggregation notebook
â”‚
â”œâ”€â”€ 03_sql/
â”‚   â”œâ”€â”€ 01_create_database.sql
â”‚   â”œâ”€â”€ 02_create_master_key.sql
â”‚   â”œâ”€â”€ 03_create_external_datasource.sql
â”‚   â”œâ”€â”€ 04_create_file_format.sql
â”‚   â”œâ”€â”€ 05_create_external_table.sql
â”‚   â””â”€â”€ 06_query_daily_revenue.sql
â”‚
â””â”€â”€ screenshots/
    â”œâ”€â”€ 01_pipeline_success.png
    â”œâ”€â”€ 02_storage_folders.png
    â”œâ”€â”€ 03_silver_notebook_output.png
    â”œâ”€â”€ 04_gold_folder.png
    â””â”€â”€ 05_sql_query_results.png
```

---

## ðŸ”„ Pipeline Walkthrough

### ðŸ¥‰ Phase 1 â€” Bronze Layer (Raw Ingestion)
- Created a **Copy Data pipeline** in the Synapse Integrate tab
- Source: HTTP dataset pointing to raw JSON retail transaction data
- Sink: ADLS Gen2, saved as **Parquet** in the `/bronze` folder
- Handled data type mapping issues (e.g. `amount` column mapped to `Double` to prevent conversion errors)

### ðŸ¥ˆ Phase 2 â€” Silver Layer (Data Cleaning with PySpark)
- Created an **Apache Spark Pool** to run PySpark notebooks
- Loaded raw Parquet from the Bronze layer
- Applied the following transformations:
  - Filtered rows to keep only `event_type = 'purchase'`
  - Dropped rows with null values in `customer_id` and `amount`
  - Converted `event_timestamp` to proper Date format using `to_date()`
  - Cast `amount` to Float for accuracy
- Saved cleaned output to `/silver` folder as Parquet

### ðŸ¥‡ Phase 3 â€” Gold Layer (Aggregation with PySpark)
- Loaded cleaned Silver data into a new notebook
- Grouped data by `event_date`
- Calculated:
  - `daily_revenue` â†’ `sum(amount)`
  - `total_purchases` â†’ `count(*)`
- Saved final aggregated output to `/gold` folder as Parquet

### ðŸ—„ï¸ Phase 4 â€” SQL Presentation Layer
- Created a dedicated database `retailpoc` in Synapse Serverless SQL Pool
- Configured security using **Database Master Key** and **Database Scoped Credentials**
- Created an **External Data Source** pointing to the ADLS Gen2 storage account
- Defined a **Parquet External File Format**
- Created an **External Table** (`daily_revenue`) mapped to the Gold layer files
- Verified results with `SELECT * FROM daily_revenue`

---

## ðŸ’¡ Key Skills Demonstrated

- **Medallion Architecture** â€” Bronze/Silver/Gold data layering
- **Pipeline Orchestration** â€” Copy Data activity with schema mapping
- **Data Type Troubleshooting** â€” Resolved ingestion errors via manual column mapping
- **PySpark Transformations** â€” Filtering, null handling, type casting, aggregations
- **Serverless SQL** â€” Cost-efficient querying with no dedicated infrastructure
- **External Tables** â€” Virtual SQL tables over Data Lake files
- **Storage Security** â€” Database Master Key, Scoped Credentials, External Data Source

---

## ðŸš€ How to Replicate This Project

1. Create an Azure Synapse Analytics workspace with an ADLS Gen2 storage account
2. Create three folders in your container: `bronze`, `silver`, `gold`
3. Run the pipeline in `01_pipeline/` to ingest raw data into the Bronze layer
4. Attach a Spark Pool and run `02_notebooks/silver_cleaning.ipynb` to clean the data
5. Run `02_notebooks/gold_aggregation.ipynb` to produce the final aggregated table
6. Run the SQL scripts in `03_sql/` in order (01 through 06) to expose the Gold data as a queryable external table

---

## ðŸ‘©â€ðŸ’» Author

**Sneha Sharon**
- ðŸ’¼ [LinkedIn](https://www.linkedin.com/in/) â† *add your LinkedIn URL here*
- ðŸ™ [GitHub](https://github.com/) â† *add your GitHub profile URL here*

---

> *This project was built as a Proof of Concept (POC) to demonstrate hands-on experience with Azure Synapse Analytics and modern data engineering practices.*
